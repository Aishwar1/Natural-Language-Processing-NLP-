{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVLbWCYNPTgz",
        "outputId": "2b58ffb7-b49c-4356-eeed-d5d161b51152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ]
        }
      ],
      "source": [
        "vocab = {}  # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n",
        "  bag = {}  # stores all of the encodings and their frequency\n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      encoding = vocab[word]  # get encoding from vocab\n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding = word_encoding  # so to keep a count of the no of times a word was used\n",
        "      word_encoding += 1   # this line and the above line isn' related to each other\n",
        "\n",
        "    if encoding in bag:  #this keeps a count here to check and add the frequency\n",
        "      bag[encoding] += 1\n",
        "    else:\n",
        "      bag[encoding] = 1\n",
        "\n",
        "  return bag\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
        "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
        "\n",
        "pos_bag = bag_of_words(positive_review)\n",
        "neg_bag = bag_of_words(negative_review)\n",
        "\n",
        "print(\"Positive:\", pos_bag)\n",
        "print(\"Negative:\", neg_bag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajuiLLNNPlEd",
        "outputId": "ec6737b9-6365-4606-9234-a9e6db0cf086"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1}\n",
            "Negative: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 21: 1, 18: 1, 19: 1, 20: 1, 17: 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "word_encoding = 1\n",
        "def one_hot_encoding(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \")\n",
        "  encoding = []\n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      code = vocab[word]\n",
        "      encoding.append(code)\n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding.append(word_encoding)\n",
        "      word_encoding += 1\n",
        "\n",
        "  return encoding\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "encoding = one_hot_encoding(text)\n",
        "print(encoding)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vl_SLHePk-7",
        "outputId": "85b84be8-758e-40e0-dea7-2699d24e00f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 1, 4, 8, 9, 2, 2, 4, 3, 3]\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
        "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
        "\n",
        "pos_encode = one_hot_encoding(positive_review)\n",
        "neg_encode = one_hot_encoding(negative_review)\n",
        "\n",
        "print(\"Positive:\", pos_encode)\n",
        "print(\"Negative:\", neg_encode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnjEVnnPPlBF",
        "outputId": "dca09981-c2bc-4fd1-c544-c694191b5755"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive: [10, 11, 12, 13, 14, 15, 5, 16, 17, 18, 19, 14, 20, 21]\n",
            "Negative: [10, 11, 12, 13, 14, 15, 5, 16, 21, 18, 19, 14, 20, 17]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBriL13-P6s-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q4spt5iAP6u8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6Xb8gVAP6yW",
        "outputId": "d0d4ffd3-7671-47fb-b7c6-b0b0508c5fe7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets look at one review\n",
        "len(train_data[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gwOlon0wf3i0",
        "outputId": "b4c7d40d-81b7-4d76-f144-30e3e1103bc7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN) # keras has this function that add and trim off the len if it's not 250"
      ],
      "metadata": {
        "id": "By_RcGrqf3lC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "MSSmcuxYf3nG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3Fbky_Df3pF",
        "outputId": "3f086d62-d4f9-4f09-84f9-9f3b77f6a995"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2843041 (10.85 MB)\n",
            "Trainable params: 2843041 (10.85 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=1, validation_split=0.2)  # not for too mab]ny epoch here cause it's taking time otherwise more epoch is great"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoZVbdq2f3sf",
        "outputId": "d7641e3a-0e88-4050-db0c-d1b8eac7b282"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 54s 80ms/step - loss: 0.4537 - acc: 0.7788 - val_loss: 0.3177 - val_acc: 0.8708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)  # two outputs one is loss and one is accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-61Vy9wSl0Vu",
        "outputId": "5dbc5775-0b4f-4f06-b5b5-c17dddf760e8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 6ms/step - loss: 0.3147 - acc: 0.8667\n",
            "[0.31473198533058167, 0.8667200207710266]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK42jDfVl0Xl",
        "outputId": "0516189b-569c-47d3-ffb1-4182fc8f1271"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# while were at it lets make a decode function\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "    PAD = 0\n",
        "    text = \"\"\n",
        "    for num in integers:\n",
        "      if num != PAD:\n",
        "        text += reverse_word_index[num] + \" \"\n",
        "\n",
        "    return text[:-1]  # this simply removes the last \" \"\n",
        "\n",
        "print(decode_integers(encoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtO4hwtjl0Zq",
        "outputId": "1a9a059b-930b-4c9b-b060-b01a1d9a323a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that movie was just amazing so amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now time to make a prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,250)) # make a pred variable for all zeros of shape 1x250 cause the data we expect is of some number of entries x 250\n",
        "  pred[0] = encoded_text # here we just join both of them the pred with the encoded_text (formed a list of encoded text)\n",
        "  result = model.predict(pred)\n",
        "  print(result[0]) # list in a list to access the first element of that list we need the indexing as to be 0\n",
        "  # print(pred)\n",
        "  if(result[0] > 0.5):\n",
        "    print('Positive review detected!')\n",
        "  else:\n",
        "    print('Negative review detected!')\n",
        "\n",
        "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kayrwL3wl0bu",
        "outputId": "f80134f3-f6c5-49d2-c819-2889a26e8cc0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n",
            "[0.8717071]\n",
            "Positive review detected!\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "[0.4540866]\n",
            "Negative review detected!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p9Mc5XHFl0ee"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Sv-uXJMl0h3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQaQa7O3wSOm",
        "outputId": "74de85c4-4b97-4ed1-c390-32b086a685fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjQ9PatIwUEI",
        "outputId": "29faf0eb-539a-4723-daf1-6f7b2fb3d8c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]"
      ],
      "metadata": {
        "id": "mg8N9US7wUFQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFwsqlqBwUGJ",
        "outputId": "cab8e636-973e-4d58-a02c-924ac4915862"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1sRHLqWwUHN",
        "outputId": "80d7a606-672a-4ce2-a09d-8c4c2807bb13"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "j-iXoxv9wUIM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLfBByhYwUJY",
        "outputId": "ff89628f-03cd-489c-a3fd-346f39412f2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34g3uc36wUKb",
        "outputId": "62fb6e1a-73cc-4bdb-9f68-6258270348fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)  # this basically tells the number of traning examples that could be taken for the text for one epoch\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "print(char_dataset) # what this did is basically sliced all the string data into stream of characters to all that 1.1 million data characters (text) is shakespeare"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4Ip5K4pwUOg",
        "outputId": "47e5de81-721c-4b81-8e94-5df2480ff690"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True) # let's say we have 105 characters in our len(text) for one time run we are going to trip it to 101\n",
        "# batch(seq_length is the length for each sequence) 101 is what is required rn (this we do because we want to map on to this afterwards in the next function)"
      ],
      "metadata": {
        "id": "oMwg8TdAwnBd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry\n",
        "# we can do the sequences by mapping them to the function"
      ],
      "metadata": {
        "id": "akuNkfiHwnD3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2): # two represents the sequences of trainable data that's 2 and x and y works for input and output statements\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F65jNXCCwnGR",
        "outputId": "9f0e09e7-1c8d-42f9-dac4-9e6c67a837af"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True) # shuffle the data is basically the data we get at a time every epoch and then batch size them"
      ],
      "metadata": {
        "id": "KbVqNoFhwnI-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeCnlSvXwnMI",
        "outputId": "76e62f29-89b7-44ef-ad4d-922b848bf132"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1): # creating a loss function\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNacfQmvwwuj",
        "outputId": "e46f3284-a5da-4673-d565-8cd138e23830"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions)) # 64 total predictions\n",
        "print(example_batch_predictions) # each of them prediction of what character could come next that is 65 character's probability of being the next character"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTLkzXHuwwwx",
        "outputId": "b8e861c8-baf7-4417-fbdc-a846f8ca57e1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-6.6562621e-03 -6.2509393e-03  6.5973583e-03 ...  6.8919417e-03\n",
            "   -2.9821561e-03  1.2209292e-03]\n",
            "  [-1.5615004e-03 -6.5179034e-03  5.1708366e-03 ...  3.6799298e-03\n",
            "   -3.3513114e-03  2.2133882e-03]\n",
            "  [ 2.6552146e-03 -1.0529354e-02  1.1287365e-02 ... -1.4594041e-03\n",
            "   -3.3430716e-03  4.5694029e-03]\n",
            "  ...\n",
            "  [ 4.0199356e-03 -7.0387921e-03 -1.2453422e-03 ...  1.7238604e-03\n",
            "   -4.5831208e-03 -1.1145329e-03]\n",
            "  [ 2.5631809e-03 -6.1245002e-03  8.1057753e-04 ...  2.8249256e-03\n",
            "   -5.0391606e-03 -1.7902120e-03]\n",
            "  [ 2.3985710e-03 -5.0807828e-03  1.3089085e-03 ...  7.8770462e-03\n",
            "   -4.7758734e-03 -2.6597013e-04]]\n",
            "\n",
            " [[-3.9371885e-03 -3.7110418e-03 -1.6933642e-03 ...  2.2689081e-03\n",
            "    7.5603893e-04 -2.9975565e-03]\n",
            "  [-6.4293495e-03  7.1477832e-04 -3.2383362e-03 ... -7.1461825e-04\n",
            "    4.5869015e-03 -2.6686373e-03]\n",
            "  [-2.7207448e-03 -4.8810616e-05 -1.4804297e-03 ...  4.0148608e-03\n",
            "    3.1729159e-03  6.6614198e-04]\n",
            "  ...\n",
            "  [-4.1151149e-03 -5.2351104e-03 -5.1607755e-03 ...  1.0226725e-02\n",
            "   -1.9986979e-03  2.7915456e-03]\n",
            "  [ 1.1634809e-03 -5.2858293e-03 -2.8265552e-03 ...  1.0087066e-02\n",
            "   -8.1256833e-03 -1.3810042e-03]\n",
            "  [ 7.0132175e-03 -3.8047116e-03 -3.4920909e-03 ...  9.8939557e-03\n",
            "   -4.1580135e-03 -1.9064150e-04]]\n",
            "\n",
            " [[-3.5914758e-03  4.8716168e-04 -2.2371302e-03 ...  4.3064449e-04\n",
            "    3.1993503e-03  5.0299279e-03]\n",
            "  [-5.1067234e-03 -6.9094636e-03 -3.2488734e-04 ...  7.7260202e-03\n",
            "   -2.6611770e-03  3.1637710e-03]\n",
            "  [-5.9907674e-04 -5.6596501e-03  1.5957698e-03 ...  7.3109404e-03\n",
            "   -7.9361955e-03 -1.3236713e-04]\n",
            "  ...\n",
            "  [-3.8898205e-03  4.5031135e-04 -2.2391318e-03 ...  7.0444676e-03\n",
            "    3.0479131e-03  7.9540899e-03]\n",
            "  [-6.1297948e-03 -6.9572870e-03 -8.9846307e-04 ...  1.4062126e-02\n",
            "   -2.4437299e-03  4.2365692e-03]\n",
            "  [-1.9874328e-03 -5.6814537e-03  6.1417604e-04 ...  1.3063282e-02\n",
            "   -7.5058518e-03 -3.3637497e-04]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.6350334e-04 -9.9849422e-04 -5.6052755e-04 ... -5.5648801e-03\n",
            "    4.6451995e-04  3.9147255e-03]\n",
            "  [-2.9667609e-03 -1.7604101e-03 -3.0899155e-03 ... -2.0978185e-03\n",
            "    2.7687470e-03  7.5592182e-04]\n",
            "  [-1.3419577e-03 -9.6593710e-04 -7.1522156e-03 ... -1.1725475e-03\n",
            "   -4.1505536e-03 -1.7951924e-03]\n",
            "  ...\n",
            "  [-2.6601152e-03 -9.5759435e-03  1.9302897e-03 ...  6.8077901e-03\n",
            "   -9.8690996e-03  1.3292723e-03]\n",
            "  [-1.8744345e-03 -4.8247585e-03  2.1289682e-03 ...  9.5465714e-03\n",
            "   -6.9951853e-03  4.7236504e-03]\n",
            "  [-1.8768786e-03 -5.2816668e-03  2.8363799e-03 ...  8.2018189e-03\n",
            "   -5.0793863e-03  3.6711523e-03]]\n",
            "\n",
            " [[ 3.3395817e-03 -2.6035234e-03 -2.2630924e-03 ...  5.9400485e-03\n",
            "   -1.6880240e-03  4.9189571e-03]\n",
            "  [ 6.5228180e-04 -4.3691192e-03  4.5599847e-04 ...  3.3806439e-03\n",
            "   -2.9284921e-03  3.5323980e-03]\n",
            "  [-7.6567261e-03 -1.0566128e-02  7.5398767e-03 ...  9.1745406e-03\n",
            "   -5.0142026e-03  2.9875825e-03]\n",
            "  ...\n",
            "  [ 1.7378386e-04 -1.0890122e-02  6.8547064e-04 ...  1.8329208e-03\n",
            "   -8.8045886e-03 -8.6076790e-04]\n",
            "  [-5.1544253e-03 -1.2432018e-02  2.2566817e-03 ...  4.1142078e-03\n",
            "   -6.3668643e-03  1.6103535e-03]\n",
            "  [-4.7223824e-03 -9.3880370e-03  1.3765655e-03 ... -3.2131516e-03\n",
            "   -5.5622328e-03  6.8319850e-03]]\n",
            "\n",
            " [[-2.1824047e-03 -2.1233342e-03  5.2370760e-04 ... -2.5102505e-03\n",
            "    1.8052782e-03  2.1572392e-03]\n",
            "  [-2.5129574e-04 -1.8783899e-03 -3.3973823e-03 ... -1.5373148e-03\n",
            "   -4.2527388e-03 -6.9631659e-04]\n",
            "  [ 4.8731724e-04 -6.1313789e-03 -6.0444167e-03 ... -3.7995556e-03\n",
            "   -4.1980552e-03  1.8107758e-03]\n",
            "  ...\n",
            "  [ 6.2553845e-03 -4.0172096e-03 -2.2716308e-04 ... -7.3414796e-04\n",
            "    4.0522371e-03  7.2124428e-03]\n",
            "  [ 3.4789757e-03 -4.5378692e-03  1.1915050e-03 ... -2.0193006e-04\n",
            "    2.4854003e-03  4.8182751e-03]\n",
            "  [-1.4854688e-03 -1.1283640e-02  2.2865166e-03 ...  6.9394568e-03\n",
            "   -2.1737106e-03  2.7727778e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9NBXRNZwwy7",
        "outputId": "88490784-ad21-4069-f480-b1b3d3a31243"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00665626 -0.00625094  0.00659736 ...  0.00689194 -0.00298216\n",
            "   0.00122093]\n",
            " [-0.0015615  -0.0065179   0.00517084 ...  0.00367993 -0.00335131\n",
            "   0.00221339]\n",
            " [ 0.00265521 -0.01052935  0.01128737 ... -0.0014594  -0.00334307\n",
            "   0.0045694 ]\n",
            " ...\n",
            " [ 0.00401994 -0.00703879 -0.00124534 ...  0.00172386 -0.00458312\n",
            "  -0.00111453]\n",
            " [ 0.00256318 -0.0061245   0.00081058 ...  0.00282493 -0.00503916\n",
            "  -0.00179021]\n",
            " [ 0.00239857 -0.00508078  0.00130891 ...  0.00787705 -0.00477587\n",
            "  -0.00026597]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-U5jBVLww2b",
        "outputId": "ccc168a7-15c8-430b-94bf-3d1ec36da13e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-6.6562621e-03 -6.2509393e-03  6.5973583e-03  3.2110182e-03\n",
            "  3.6946475e-04 -3.7371381e-03  2.4953049e-03  1.3800042e-03\n",
            " -2.4038402e-04  8.3821360e-04  4.4876309e-03  1.0372584e-03\n",
            " -1.5536032e-04  3.9805616e-03 -1.5067154e-03 -9.6787128e-04\n",
            " -1.4555521e-04 -5.1938826e-03  2.6772509e-04 -2.1575596e-03\n",
            "  3.4398115e-03  8.2159491e-04  2.2287644e-03 -2.5640614e-03\n",
            "  6.5980450e-04  1.3191384e-03  8.8796054e-04  1.0127691e-03\n",
            "  5.3809648e-03 -5.0393012e-03  6.6760695e-05  3.4107747e-03\n",
            " -9.6277483e-03  3.1510717e-04 -6.8989764e-03 -2.4129963e-03\n",
            "  5.2518877e-03  3.9425981e-03  1.8721852e-03 -4.5279245e-04\n",
            "  3.5685652e-03 -1.6247621e-03  6.5710105e-04  2.5349073e-03\n",
            " -3.3043926e-03 -1.5282098e-03  2.8020819e-03 -9.2649140e-04\n",
            " -1.9404087e-03  6.1734521e-05  5.2349092e-03  6.4825905e-03\n",
            " -3.6591985e-03  2.7226366e-04  2.6539369e-03  3.2993773e-04\n",
            "  2.1286928e-03 -1.1044986e-03 -9.7449170e-04  1.8042559e-04\n",
            " -4.9414858e-03  1.9832575e-03  6.8919417e-03 -2.9821561e-03\n",
            "  1.2209292e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1) # we took the random character\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices) # we sampled it cause it's not always that the most common is the one that's going to be the next character\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1 # this isn't trained that's why the result here is awful"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nrG8lxjFwxBA",
        "outputId": "78c4deb9-89e5-453f-d683-6922d0d48b49"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Fy'uYIBYsoWcBmZvh:zPaAzwotP Kfn,e,OiZt;HBLse'PT,&oqy.zvQZsTLiOLoktJ;jTRRNnG$R,TuXBuUkb&CbHgMha&dDYTj\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits): # build in loss function from keras (logits is the probability distribution)\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "O3qdMnGewxEf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "JEx_T6ShxCF6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, # checkpoint\n",
        "    save_weights_only=True)\n",
        "# This will allow us to load our model from a checkpoint & continue training it cause we have created a epoch of 50 and it can go further to 100 as wellwhich will take alot of time"
      ],
      "metadata": {
        "id": "JnQXN6UUxCAh"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=1, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "hyJxR0jkxCCe"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "9b5TQa9zxIHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) #loading checkpoint the latest one\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "B52Lse9IxIJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None])) # here loading checkpoint any intermedite one (shape is basically one sequence and the length of sequence is not defined)"
      ],
      "metadata": {
        "id": "ELEdFGhfxILW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0) # increase the dimention of it cause that's what it expects so earlier it was [1,2,4,5,6] not it's [[1,2,4,5,6]]\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()  # reset it cause when we rebuild the model it had saved or stored the data of last state that it remembered when it was training\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "\n",
        "      predictions = tf.squeeze(predictions, 0) # removes the extra outer dimention of the prediction that we'll get\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy() # will give us the that random which it feels is to be given (categorical distribution)\n",
        "      # predicted_id is the sampled output by the model which it has learned in it's training\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0) # and that we'll going to add that to the input eval which is basically (inp) itself in idx form\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id]) # then append it to an empty list and convert it to the character it is\n",
        "\n",
        "  return (start_string + ''.join(text_generated)) # then join it to the starting string that's (inp)"
      ],
      "metadata": {
        "id": "m06cL30axINR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "id": "IlqqkCBZxIPM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}